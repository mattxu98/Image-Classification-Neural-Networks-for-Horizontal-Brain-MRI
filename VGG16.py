# -*- coding: utf-8 -*-
"""VGG16_3ver.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/109HBnWIeEm_quY9GpVwI4h7nR-4cThoc
"""

import torch
import torch.nn as nn

from google.colab import drive
drive.mount('/content/drive')

from torchvision import transforms
class AddGaussNoise(object):
    def __init__(self, mean=0.0, std=1.):
        self._mean = mean
        self._std = std
    def __call__(self, tensor):
        return tensor + torch.randn(tensor.size()) * self._std \
        + self._mean
    def __repr__(self):
        return self.__class__.__name__ + '(mean={0}, std={1})'.\
        format(self._mean, self._std)
transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    AddGaussNoise(0.0, 0.1)]) # noise ~ N(0, 0.1)

from torchvision.datasets import ImageFolder
# image #file placement format
# data
#  ｜
#  ｜--no
#  ｜
#  ｜--yes
#  ｜
dataset = ImageFolder(root='/content/drive/MyDrive/Colab Notebooks/ML-ASS3/archive/', transform=transform)



dataset

from torch.utils.data import random_split
train_size = int(0.8 * len(dataset))
val_size = int(0.1 * len(dataset))
test_size = len(dataset) - train_size - val_size
train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])

from torch.utils.data import DataLoader
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,pin_memory = True, num_workers=12) #16
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False,pin_memory = True, num_workers=12) #16
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False,pin_memory = True, num_workers=12) #16

#Version001: pretrained=True Optimizer SGD + param.requires_grad = True = 0.9967
import torchvision.models as models
vgg16 = models.vgg16(pretrained=True) #load pre-trained VGG model #transfer learning
features = list(vgg16.classifier.children())[:-1]
#Obtain the classifier part of VGG16 (excluding the last layer of the fully connected layers)
#Add a new fully connected layer
features.extend([
    torch.nn.Linear(4096, 1024),
    torch.nn.ReLU(inplace=True),
    torch.nn.Dropout(p=0.5),
    torch.nn.Linear(1024, 1024),
    torch.nn.ReLU(inplace=True),
    torch.nn.Dropout(p=0.5),
    torch.nn.Linear(1024, 512),
    torch.nn.ReLU(inplace=True),
    torch.nn.Dropout(p=0.5),
    torch.nn.Linear(512, 2)])  #Output layer with 2 classes
vgg16.classifier = torch.nn.Sequential(*features) #replace classifier
for param in vgg16.classifier.parameters(): #unfreeze all parameters
    param.requires_grad = True
model_vgg16_pretrained_unfreeze_p = vgg16
#print(model)

#Version002: Rewrite + nn.Flatten + Optimizer SGD + param.requires_grad = False = 0.9967
import torchvision.models as models
vgg16 = models.vgg16(pretrained=True)  #load pre-trained VGG
vgg16 = vgg16.features #Remove the fully connected layers (include_top=False)
class fully_c_layer(nn.Module): # Define the fully connected layer
    def __init__(self, num_classes):
        super(fully_c_layer, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d((7, 7))
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(512 * 7 * 7, 1024),
            nn.ReLU(inplace=True),
            nn.Linear(1024, 1024),
            nn.ReLU(inplace=True),
            nn.Linear(1024, 512),
            nn.ReLU(inplace=True),
            nn.Linear(512, num_classes),)
    def forward(self, x):
        x = self.avg_pool(x)
        x = self.classifier(x)
        return x
num_classes = 2 # Number of classes final
for param in vgg16.parameters(): #freeze the layers
    param.requires_grad = False
#combine the feature extraction layers (vgg16) and my fully_c_layer
model_vgg16_pretrained_freeze_p = nn.Sequential(vgg16,fully_c_layer(num_classes))

#Version003: Scratch
vgg16 = models.vgg16(pretrained=False) #train from scratch
vgg16_features = vgg16.features
class fully_c_layer(nn.Module):
    def __init__(self, num_classes):
        super(fully_c_layer, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d((7, 7))
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(512 * 7 * 7, 1024),
            #nn.ReLU(inplace=True),
            #nn.Sigmoid(),
            nn.Tanh(),
            nn.Linear(1024, 1024),
            #nn.ReLU(inplace=True),
            #nn.Sigmoid(),
            nn.Tanh(),
            nn.Linear(1024, 512),
            #nn.ReLU(inplace=True),
            #nn.Sigmoid(),
            nn.Tanh(),
            nn.Linear(512, num_classes),)
    def forward(self, x):
        x = self.avg_pool(x)
        x = self.classifier(x)
        return x
num_classes = 2  # Number of classes final
model_vgg16_scratch = nn.Sequential(vgg16_features, fully_c_layer(num_classes))

#EarlyStopping:patience(steps),min_delta(min loss),__call__(reset step and loss)
class EarlyStopping:
    def __init__(self, patience=100, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.stop = False
    def __call__(self, loss):
        if self.best_loss is None:
            self.best_loss = loss
        elif self.best_loss - loss > self.min_delta:
            self.best_loss = loss
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.stop = True

from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score
#initialize lists to store metrics
train_losses = []
val_losses = []
recalls = []
f1s = []
accuracies = []

#decide which model & optimizer
#model = model_vgg16_pretrained_unfreeze_p
#model = model_vgg16_pretrained_freeze_p
model = model_vgg16_scratch
#optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
#optimizer = torch.optim.SGD(model.parameters(), lr=0.1)

criterion = nn.CrossEntropyLoss() #loss
num_epochs = 300 #epoch num.
early_stopping = EarlyStopping(patience=10, min_delta=0.001) #
best_val_loss = float('inf') #initialized a loss
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") #GPU
print("Device:", device)
print(torch.cuda.is_available())
model = model.to(device)

import logging
for handler in logging.root.handlers[:]: #clear all exisinting log handlers
    logging.root.removeHandler(handler)
logging.basicConfig(filename='training.log', level=logging.INFO,#config logging
                    format='%(asctime)s - %(message)s')
console = logging.StreamHandler()
console.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(message)s')
console.setFormatter(formatter)
logging.getLogger('').addHandler(console)

for epoch in range(num_epochs): #start training
    model.train() #training
    total_loss = 0.0
    for imgs, labels in train_loader:
        imgs, labels = imgs.to(device), labels.to(device) #GPU
        optimizer.zero_grad() # Clear gradients
        outputs = model(imgs) # Forward pass
        loss = criterion(outputs, labels) # Calculate loss
        loss.backward() # Backward pass
        optimizer.step()  # Update weights
        total_loss += loss.item()

    avg_train_loss = total_loss / len(train_loader)
    train_losses.append(avg_train_loss)
    logging.info(f"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}")
    validate_every = 1  #validate every epoch
    if (epoch + 1) % validate_every == 0:
        model.eval() #validate
        total_val_loss = 0.0
        # total_correct = 0
        # total_samples = 0
        all_labels = []  #collecting all readl labels
        all_predictions = []  #collecting all predicted labels
        with torch.no_grad():
            for imgs, labels in val_loader:
                imgs, labels = imgs.to(device), labels.to(device)
                outputs = model(imgs)
                _, predicted = torch.max(outputs, 1)
                all_labels.extend(labels.cpu().numpy())
                all_predictions.extend(predicted.cpu().numpy())
                loss = criterion(outputs, labels)
                total_val_loss += loss.item()
                # total_samples += labels.size(0)
                # total_correct += (predicted == labels).sum().item()
        # accuracy = 100.0 * total_correct / total_samples
        accuracy = accuracy_score(all_labels, all_predictions)
        recall = recall_score(all_labels, all_predictions)
        precision = precision_score(all_labels, all_predictions)
        f1 = f1_score(all_labels, all_predictions)
        val_loss = total_val_loss / len(val_loader)
        val_losses.append(val_loss)
        recalls.append(recall)
        f1s.append(f1)
        accuracies.append(accuracy)
        logging.info(f"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}\n \
                       Recall: {recall:.4f}, Precision: {precision:.4f}, F1 Score: {f1:.4f}, Accuracy: {accuracy:.4f}")
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), 'best_model.pth')

    early_stopping(avg_train_loss)
    if early_stopping.stop:
        torch.save(model.state_dict(), 'best_last_model.pth')
        logging.info("Early stopping triggered.")
        break

torch.save(model.state_dict(), 'last_model.pth')

#plot the metrics
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
import numpy as np

epochs = range(1, len(train_losses) + 1)
metrics = [
    ('Train loss and test loss', train_losses, val_losses, 'Loss'),
    ('Recall', recalls, None, 'Recall'),  #none indicates no second series to plot
    ('f1', f1s, None, 'F1 Score'),
    ('Accuracy', accuracies, None, 'Accuracy')]

plt.figure(figsize=(20, 4))
for i, (title, y1, y2, ylabel) in enumerate(metrics, 1):
    plt.subplot(1, 4, i)
    if y2 is not None:
        plt.plot(epochs, y1, label='Train Loss')
        plt.plot(epochs, y2, label='Test Loss')
        plt.legend()
    else:
        plt.plot(epochs, y1, label=title)
    plt.title(f"{model.__class__.__name__}: {title}")
    plt.xlabel('Epochs')
    plt.ylabel(ylabel)
plt.tight_layout()
# plt.savefig(f"metrics_plots_{model.__class__.__name__}.png", dpi=300)  # Save figure as PNG
plt.show()

#calculate confusion matrix
plt.figure(figsize=(3.5, 2))

cm = confusion_matrix(all_labels, all_predictions).T
cm = np.flipud(cm)  #flip rows
cm = np.fliplr(cm)  #flip columns
ax = sns.heatmap(cm, annot=True, fmt='d')

cbar = ax.collections[0].colorbar  #colorbar from the axis object
cbar.ax.tick_params(labelsize=7)  #fontsize for colorbar tick labels

plt.ylabel('Predicted Label', fontsize=7)
ax.tick_params(axis='y', labelsize=7)
ax.set_yticklabels(['Positive', 'Negative'])  #flip the tick labels

ax = plt.gca()  #current axis
ax.xaxis.set_label_position('top')  #x-label to be top
plt.xlabel('Actual Label', fontsize=7)
ax.xaxis.tick_top()  #x-axis ticks to be top
ax.set_xticklabels(['Positive', 'Negative'])  #tick labels
ax.tick_params(axis='x', labelsize=7)

plt.title(f"{model.__class__.__name__}: Confusion Matrix", fontsize=9)
# plt.savefig(f"confusion_matrix_{model.__class__.__name__}.png", dpi=300)  # Save figure as PNG
plt.show()